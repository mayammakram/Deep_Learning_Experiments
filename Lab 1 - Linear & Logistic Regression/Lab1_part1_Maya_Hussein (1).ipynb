{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HNdarz1uIIZ"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/sherifmost/DeepLearning/blob/master/Labs/lab1/lab1_part1.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSA7aTOivRhA"
      },
      "source": [
        "**Parts of this lab are based on Kaggle kernels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfATj8pltlxh"
      },
      "source": [
        "# Lab 1 - Part1: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOy1mYdKw5TE"
      },
      "source": [
        "![Linear Regression](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/linear_regression.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsLKJ-4kwLYv"
      },
      "source": [
        "## 1.1.1 Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxlb182u_gv"
      },
      "source": [
        "The problem we are trying to solve here is finding a new house which is suitable to our needs and the budget we assigned. The client who wants to buy the new house did her research and found some houses. She wrote the details of each house she visited including location, sale condition, sale type, house price, among others. She needs some help to know how much she is expected to pay to get a house that conforms with her specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUidwfyPvcfv"
      },
      "source": [
        "Your task is to build a linear regression model that helps her to predict the house price depending on the given attributes she collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeXilc2Nvg-g"
      },
      "source": [
        "## 1.1.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsHAbLgL1MsY"
      },
      "source": [
        "Let's dive into the code, explain it and show you the parts you need to fill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n5JS__dwfiH"
      },
      "source": [
        "### 1.1.2.1 Import Needed packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Xm9MPDVSsDpu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy.stats.stats import pearsonr\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EfciecZwm6L"
      },
      "source": [
        "### 1.1.2.2 Configure Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EW1TyciwsN9_"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PJSAiyx4HM"
      },
      "source": [
        "### 1.1.2.3 Work on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcVsD6V1x-Ty"
      },
      "source": [
        "This dataset contains 80 features that demonstrate the state of the house and our target which is the house price.\n",
        "\n",
        "We begin by loading the train and test splits of the dataset using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "P6zYNeXVvvRA"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyl1lDq-yVFG"
      },
      "source": [
        "You can have a look at the train split of the dataset using the head command. I very much encourage you to have a deeper look on the dataset file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "b2JWObzgyY9t",
        "outputId": "87e315bb-488c-4534-90f2-a1f7b867c0a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
              "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
              "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
              "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
              "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
              "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
              "\n",
              "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
              "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
              "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
              "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
              "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
              "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
              "\n",
              "  YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0   2008        WD         Normal     208500  \n",
              "1   2007        WD         Normal     181500  \n",
              "2   2008        WD         Normal     223500  \n",
              "3   2006        WD        Abnorml     140000  \n",
              "4   2008        WD         Normal     250000  \n",
              "\n",
              "[5 rows x 81 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e38ac9d-ea30-4192-bb18-db2c2eb8da6d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>...</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 81 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e38ac9d-ea30-4192-bb18-db2c2eb8da6d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e38ac9d-ea30-4192-bb18-db2c2eb8da6d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e38ac9d-ea30-4192-bb18-db2c2eb8da6d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXVoqAU0h1g"
      },
      "source": [
        "Data preprocessing:\n",
        "* First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal\n",
        "* Create Dummy variables for the categorical features\n",
        "* Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nCCi1svksOj4"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the data\n",
        "# We do this to be able to preprocess on the whole dataset\n",
        "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
        "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
        "\n",
        "# Log transform the target y in training data - by reference inside all\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "\n",
        "# Log transform skewed numeric features:\n",
        "\n",
        "# Get Numerical Fields\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index \n",
        "\n",
        "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewnessc\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75] # Get Skewed Columns\n",
        "skewed_feats = skewed_feats.index # Get Skewed Columns indices\n",
        "\n",
        "# Log scale skewed columns\n",
        "# Normalize the skewed distribution for better regression\n",
        "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
        "\n",
        "# Create Dummy variables for the categorical features \n",
        "all_data = pd.get_dummies(all_data) \n",
        "\n",
        "# Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "all_data = all_data.fillna(all_data.mean())\n",
        "\n",
        "# Split the data to training & testing\n",
        "X_train = all_data[:train.shape[0]]\n",
        "X_test = all_data[train.shape[0]:]\n",
        "y = train.SalePrice\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "# z = (x - u) / s\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "#split training data into training & validation, default splitting is 25% validation\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVa9K1Ga1vlo"
      },
      "source": [
        "### 1.1.2.4 Define your model here (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IocosMrh2W3X"
      },
      "source": [
        "One important note you need to be aware of, linear regression is a neural network with only one perceptron (i.e. dense layer with one node) with a linear activation (i.e. no activation function). \n",
        "\n",
        "![One Perceptron Neural Network](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/perceptron.png)\n",
        "\n",
        "Use this note to define a **sequential model of one dense layer with one unit using Tensorflow.Keras**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Oh1NvkNXsVVG"
      },
      "outputs": [],
      "source": [
        "#regularizers = l2(0.01)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, kernel_regularizer =l2(0.01)))\n",
        "\n",
        "# TODO: Define the Model using Tensorflow.Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSa178n-2BM-"
      },
      "source": [
        "### 1.1.2.5 Compile your model and print a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fg6Yy5oDsXyK"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = \"mean_squared_error\", optimizer = \"Adam\")\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgD-Kqo3fwb"
      },
      "source": [
        "### 1.1.2.6 Train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA4pLcN73waK"
      },
      "source": [
        "Fit your model into the training data, use the validation data to be able to plot the loss decrement during the training. \n",
        "\n",
        "**Make sure to handle overfitting (check the [FAQ](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab1/FAQ.md))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAkW1KTm3TGU",
        "outputId": "a696e483-47d4-4c4a-8b3f-a2add50a8e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "35/35 [==============================] - 1s 6ms/step - loss: 146.2793 - val_loss: 147.5055\n",
            "Epoch 2/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 144.3248 - val_loss: 148.6910\n",
            "Epoch 3/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 142.8107 - val_loss: 149.9202\n",
            "Epoch 4/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 141.3470 - val_loss: 151.3042\n",
            "Epoch 5/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 139.9065 - val_loss: 152.8376\n",
            "Epoch 6/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 138.5037 - val_loss: 154.5784\n",
            "Epoch 7/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 137.0938 - val_loss: 156.0670\n",
            "Epoch 8/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 135.7928 - val_loss: 157.7786\n",
            "Epoch 9/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 134.4723 - val_loss: 159.6941\n",
            "Epoch 10/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 133.2457 - val_loss: 161.4542\n",
            "Epoch 11/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 131.9775 - val_loss: 163.4949\n",
            "Epoch 12/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 130.8128 - val_loss: 165.2845\n",
            "Epoch 13/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 129.6270 - val_loss: 167.5164\n",
            "Epoch 14/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 128.4336 - val_loss: 169.7119\n",
            "Epoch 15/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 127.2846 - val_loss: 171.8750\n",
            "Epoch 16/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 126.1311 - val_loss: 174.3416\n",
            "Epoch 17/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 125.0534 - val_loss: 176.8761\n",
            "Epoch 18/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 123.8892 - val_loss: 179.2560\n",
            "Epoch 19/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 122.8160 - val_loss: 181.8727\n",
            "Epoch 20/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 121.7406 - val_loss: 184.4680\n",
            "Epoch 21/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 120.6679 - val_loss: 187.3522\n",
            "Epoch 22/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 119.6045 - val_loss: 190.2503\n",
            "Epoch 23/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 118.5675 - val_loss: 193.2852\n",
            "Epoch 24/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 117.5428 - val_loss: 196.2696\n",
            "Epoch 25/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 116.5205 - val_loss: 199.1285\n",
            "Epoch 26/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 115.5242 - val_loss: 202.4870\n",
            "Epoch 27/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 114.5029 - val_loss: 205.5544\n",
            "Epoch 28/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 113.5521 - val_loss: 208.8722\n",
            "Epoch 29/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 112.5720 - val_loss: 212.2034\n",
            "Epoch 30/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 111.5705 - val_loss: 215.6323\n",
            "Epoch 31/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 110.6314 - val_loss: 219.1847\n",
            "Epoch 32/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 109.6712 - val_loss: 222.7170\n",
            "Epoch 33/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 108.7435 - val_loss: 226.3264\n",
            "Epoch 34/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 107.7923 - val_loss: 230.1404\n",
            "Epoch 35/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 106.8787 - val_loss: 234.1611\n",
            "Epoch 36/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 106.0033 - val_loss: 238.0145\n",
            "Epoch 37/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 105.0577 - val_loss: 241.9931\n",
            "Epoch 38/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 104.1563 - val_loss: 245.9359\n",
            "Epoch 39/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 103.2451 - val_loss: 250.2577\n",
            "Epoch 40/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 102.3837 - val_loss: 254.4534\n",
            "Epoch 41/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 101.5351 - val_loss: 258.8753\n",
            "Epoch 42/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 100.6290 - val_loss: 263.3044\n",
            "Epoch 43/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 99.7950 - val_loss: 267.4527\n",
            "Epoch 44/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 98.9278 - val_loss: 271.9987\n",
            "Epoch 45/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 98.0669 - val_loss: 276.3812\n",
            "Epoch 46/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 97.2446 - val_loss: 281.0776\n",
            "Epoch 47/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 96.3863 - val_loss: 285.3936\n",
            "Epoch 48/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 95.6088 - val_loss: 290.3216\n",
            "Epoch 49/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 94.7555 - val_loss: 295.0107\n",
            "Epoch 50/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 93.9447 - val_loss: 300.0071\n",
            "Epoch 51/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 93.1386 - val_loss: 304.9168\n",
            "Epoch 52/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 92.3509 - val_loss: 309.8470\n",
            "Epoch 53/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 91.5623 - val_loss: 314.9475\n",
            "Epoch 54/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 90.7619 - val_loss: 319.8912\n",
            "Epoch 55/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 90.0080 - val_loss: 325.1285\n",
            "Epoch 56/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 89.2021 - val_loss: 330.3146\n",
            "Epoch 57/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 88.4437 - val_loss: 335.6248\n",
            "Epoch 58/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 87.6713 - val_loss: 341.0090\n",
            "Epoch 59/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 86.9165 - val_loss: 346.6184\n",
            "Epoch 60/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 86.1822 - val_loss: 351.6971\n",
            "Epoch 61/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 85.4155 - val_loss: 357.3152\n",
            "Epoch 62/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 84.7098 - val_loss: 362.9557\n",
            "Epoch 63/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.9750 - val_loss: 368.7605\n",
            "Epoch 64/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.2523 - val_loss: 374.4672\n",
            "Epoch 65/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 82.5140 - val_loss: 379.9726\n",
            "Epoch 66/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 81.7931 - val_loss: 385.8589\n",
            "Epoch 67/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 81.0821 - val_loss: 391.7173\n",
            "Epoch 68/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 80.3838 - val_loss: 397.6601\n",
            "Epoch 69/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 79.6668 - val_loss: 403.8747\n",
            "Epoch 70/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 78.9920 - val_loss: 409.2461\n",
            "Epoch 71/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 78.2755 - val_loss: 415.2898\n",
            "Epoch 72/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 77.6021 - val_loss: 421.4392\n",
            "Epoch 73/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 76.9566 - val_loss: 427.2006\n",
            "Epoch 74/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 76.2456 - val_loss: 433.5502\n",
            "Epoch 75/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 75.5388 - val_loss: 440.0306\n",
            "Epoch 76/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 74.8978 - val_loss: 446.4052\n",
            "Epoch 77/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 74.2208 - val_loss: 452.3713\n",
            "Epoch 78/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 73.5947 - val_loss: 459.2836\n",
            "Epoch 79/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 72.8973 - val_loss: 465.7931\n",
            "Epoch 80/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 72.2291 - val_loss: 471.6833\n",
            "Epoch 81/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 71.6242 - val_loss: 478.3587\n",
            "Epoch 82/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 70.9436 - val_loss: 485.2346\n",
            "Epoch 83/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 70.3123 - val_loss: 492.0366\n",
            "Epoch 84/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 69.6867 - val_loss: 498.7700\n",
            "Epoch 85/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 69.0549 - val_loss: 505.4742\n",
            "Epoch 86/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 68.4375 - val_loss: 512.3804\n",
            "Epoch 87/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 67.8098 - val_loss: 519.6402\n",
            "Epoch 88/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 67.1923 - val_loss: 526.5887\n",
            "Epoch 89/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.6145 - val_loss: 533.6507\n",
            "Epoch 90/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 65.9679 - val_loss: 541.1149\n",
            "Epoch 91/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 65.3842 - val_loss: 547.9586\n",
            "Epoch 92/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 64.7726 - val_loss: 555.3986\n",
            "Epoch 93/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 64.1862 - val_loss: 562.8853\n",
            "Epoch 94/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 63.5980 - val_loss: 569.9761\n",
            "Epoch 95/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 63.0148 - val_loss: 577.4384\n",
            "Epoch 96/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.4238 - val_loss: 584.7816\n",
            "Epoch 97/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 61.8488 - val_loss: 592.0699\n",
            "Epoch 98/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 61.2701 - val_loss: 599.2599\n",
            "Epoch 99/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 60.6947 - val_loss: 607.0201\n",
            "Epoch 100/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 60.1517 - val_loss: 614.6767\n",
            "Epoch 101/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.5639 - val_loss: 622.2667\n",
            "Epoch 102/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.0044 - val_loss: 630.2599\n",
            "Epoch 103/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.4633 - val_loss: 637.6749\n",
            "Epoch 104/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 57.9119 - val_loss: 645.5803\n",
            "Epoch 105/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 57.3499 - val_loss: 653.5353\n",
            "Epoch 106/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.8255 - val_loss: 661.2589\n",
            "Epoch 107/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.2670 - val_loss: 669.3301\n",
            "Epoch 108/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.7070 - val_loss: 676.7245\n",
            "Epoch 109/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 55.2215 - val_loss: 684.9700\n",
            "Epoch 110/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 54.6625 - val_loss: 693.2498\n",
            "Epoch 111/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.1549 - val_loss: 701.4427\n",
            "Epoch 112/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.6115 - val_loss: 709.2817\n",
            "Epoch 113/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 53.1139 - val_loss: 717.3923\n",
            "Epoch 114/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 52.5949 - val_loss: 725.9465\n",
            "Epoch 115/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.0658 - val_loss: 734.4977\n",
            "Epoch 116/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 51.5762 - val_loss: 742.6277\n",
            "Epoch 117/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 51.0752 - val_loss: 751.0378\n",
            "Epoch 118/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 50.5751 - val_loss: 759.5666\n",
            "Epoch 119/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 50.0705 - val_loss: 767.7045\n",
            "Epoch 120/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 49.5795 - val_loss: 776.0847\n",
            "Epoch 121/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 49.0830 - val_loss: 784.4894\n",
            "Epoch 122/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.6560 - val_loss: 793.0164\n",
            "Epoch 123/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.1454 - val_loss: 801.7072\n",
            "Epoch 124/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 47.6517 - val_loss: 809.5507\n",
            "Epoch 125/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.1853 - val_loss: 818.2599\n",
            "Epoch 126/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.7172 - val_loss: 827.1729\n",
            "Epoch 127/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.2473 - val_loss: 835.8455\n",
            "Epoch 128/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.7788 - val_loss: 844.5831\n",
            "Epoch 129/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.3112 - val_loss: 853.0688\n",
            "Epoch 130/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.8697 - val_loss: 862.1796\n",
            "Epoch 131/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.3925 - val_loss: 871.0645\n",
            "Epoch 132/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 43.9665 - val_loss: 879.6934\n",
            "Epoch 133/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 43.5116 - val_loss: 888.7879\n",
            "Epoch 134/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 43.0553 - val_loss: 897.6618\n",
            "Epoch 135/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.6437 - val_loss: 907.1291\n",
            "Epoch 136/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.1849 - val_loss: 916.0305\n",
            "Epoch 137/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.7562 - val_loss: 925.0912\n",
            "Epoch 138/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.3371 - val_loss: 934.3110\n",
            "Epoch 139/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.8958 - val_loss: 943.7791\n",
            "Epoch 140/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 40.4831 - val_loss: 952.7613\n",
            "Epoch 141/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.0538 - val_loss: 961.4883\n",
            "Epoch 142/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 39.6178 - val_loss: 970.8299\n",
            "Epoch 143/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.2150 - val_loss: 979.7232\n",
            "Epoch 144/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.8120 - val_loss: 988.9864\n",
            "Epoch 145/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.3867 - val_loss: 998.4734\n",
            "Epoch 146/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.9869 - val_loss: 1007.7095\n",
            "Epoch 147/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.6049 - val_loss: 1016.9680\n",
            "Epoch 148/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.2030 - val_loss: 1026.0457\n",
            "Epoch 149/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.7900 - val_loss: 1035.7114\n",
            "Epoch 150/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.3864 - val_loss: 1045.4075\n",
            "Epoch 151/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.0000 - val_loss: 1054.4929\n",
            "Epoch 152/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 35.6209 - val_loss: 1064.4055\n",
            "Epoch 153/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 35.2583 - val_loss: 1073.2108\n",
            "Epoch 154/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 34.8563 - val_loss: 1082.9141\n",
            "Epoch 155/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.4774 - val_loss: 1092.3917\n",
            "Epoch 156/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.0962 - val_loss: 1102.1317\n",
            "Epoch 157/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.7319 - val_loss: 1111.4410\n",
            "Epoch 158/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.3881 - val_loss: 1120.7207\n",
            "Epoch 159/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.0129 - val_loss: 1130.5492\n",
            "Epoch 160/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 32.6327 - val_loss: 1140.4463\n",
            "Epoch 161/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.3090 - val_loss: 1149.5510\n",
            "Epoch 162/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.9270 - val_loss: 1159.2690\n",
            "Epoch 163/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 31.5806 - val_loss: 1168.5220\n",
            "Epoch 164/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.2480 - val_loss: 1178.4030\n",
            "Epoch 165/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.9061 - val_loss: 1187.3595\n",
            "Epoch 166/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.5568 - val_loss: 1197.4972\n",
            "Epoch 167/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.2060 - val_loss: 1207.1105\n",
            "Epoch 168/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 29.8591 - val_loss: 1216.5885\n",
            "Epoch 169/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.5183 - val_loss: 1225.5383\n",
            "Epoch 170/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.1969 - val_loss: 1235.1630\n",
            "Epoch 171/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 28.8804 - val_loss: 1244.9839\n",
            "Epoch 172/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.5636 - val_loss: 1254.7274\n",
            "Epoch 173/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.2462 - val_loss: 1264.0343\n",
            "Epoch 174/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.9221 - val_loss: 1273.9629\n",
            "Epoch 175/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.5998 - val_loss: 1283.3625\n",
            "Epoch 176/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.2597 - val_loss: 1291.5696\n",
            "Epoch 177/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.9588 - val_loss: 1300.8398\n",
            "Epoch 178/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.6583 - val_loss: 1310.8164\n",
            "Epoch 179/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.3465 - val_loss: 1320.1835\n",
            "Epoch 180/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.0467 - val_loss: 1329.7659\n",
            "Epoch 181/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.7548 - val_loss: 1339.4574\n",
            "Epoch 182/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 25.4577 - val_loss: 1349.4161\n",
            "Epoch 183/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.1825 - val_loss: 1358.3387\n",
            "Epoch 184/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.8741 - val_loss: 1368.1919\n",
            "Epoch 185/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.5872 - val_loss: 1377.9131\n",
            "Epoch 186/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.3010 - val_loss: 1387.1354\n",
            "Epoch 187/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.0320 - val_loss: 1396.7300\n",
            "Epoch 188/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.7369 - val_loss: 1406.1161\n",
            "Epoch 189/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.4555 - val_loss: 1414.9392\n",
            "Epoch 190/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 23.1849 - val_loss: 1424.7178\n",
            "Epoch 191/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.9117 - val_loss: 1433.5868\n",
            "Epoch 192/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.6582 - val_loss: 1443.2697\n",
            "Epoch 193/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.3887 - val_loss: 1452.0256\n",
            "Epoch 194/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.1262 - val_loss: 1461.4446\n",
            "Epoch 195/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 21.8552 - val_loss: 1470.6464\n",
            "Epoch 196/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.6141 - val_loss: 1480.2593\n",
            "Epoch 197/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.3383 - val_loss: 1489.0237\n",
            "Epoch 198/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 21.0778 - val_loss: 1497.9812\n",
            "Epoch 199/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.8394 - val_loss: 1507.4293\n",
            "Epoch 200/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.5884 - val_loss: 1516.2759\n",
            "Epoch 201/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.3517 - val_loss: 1525.4783\n",
            "Epoch 202/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.1052 - val_loss: 1534.3427\n",
            "Epoch 203/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.8628 - val_loss: 1543.5770\n",
            "Epoch 204/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.6529 - val_loss: 1551.7295\n",
            "Epoch 205/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.3831 - val_loss: 1560.4093\n",
            "Epoch 206/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.1509 - val_loss: 1568.9478\n",
            "Epoch 207/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.9315 - val_loss: 1577.5781\n",
            "Epoch 208/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.7011 - val_loss: 1586.1259\n",
            "Epoch 209/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.4733 - val_loss: 1594.4570\n",
            "Epoch 210/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.2534 - val_loss: 1602.7198\n",
            "Epoch 211/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.0312 - val_loss: 1611.2760\n",
            "Epoch 212/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.8093 - val_loss: 1620.1549\n",
            "Epoch 213/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.6073 - val_loss: 1627.8806\n",
            "Epoch 214/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.3957 - val_loss: 1635.8533\n",
            "Epoch 215/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.1745 - val_loss: 1643.8232\n",
            "Epoch 216/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 16.9672 - val_loss: 1651.6204\n",
            "Epoch 217/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.7636 - val_loss: 1659.3975\n",
            "Epoch 218/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.5629 - val_loss: 1667.3597\n",
            "Epoch 219/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.3616 - val_loss: 1674.4319\n",
            "Epoch 220/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.1580 - val_loss: 1682.9332\n",
            "Epoch 221/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.9711 - val_loss: 1690.2429\n",
            "Epoch 222/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.7707 - val_loss: 1697.4438\n",
            "Epoch 223/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.5887 - val_loss: 1704.0354\n",
            "Epoch 224/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.4009 - val_loss: 1711.2083\n",
            "Epoch 225/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 15.2059 - val_loss: 1717.9196\n",
            "Epoch 226/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.0282 - val_loss: 1725.0706\n",
            "Epoch 227/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.8634 - val_loss: 1731.8413\n",
            "Epoch 228/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.6582 - val_loss: 1738.5490\n",
            "Epoch 229/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.4844 - val_loss: 1744.9227\n",
            "Epoch 230/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.3137 - val_loss: 1751.4167\n",
            "Epoch 231/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.1379 - val_loss: 1756.9077\n",
            "Epoch 232/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.9664 - val_loss: 1762.8784\n",
            "Epoch 233/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.7962 - val_loss: 1769.1042\n",
            "Epoch 234/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.6331 - val_loss: 1774.5798\n",
            "Epoch 235/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.4606 - val_loss: 1779.7559\n",
            "Epoch 236/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2975 - val_loss: 1784.8474\n",
            "Epoch 237/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1371 - val_loss: 1790.0510\n",
            "Epoch 238/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9915 - val_loss: 1794.4706\n",
            "Epoch 239/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.8313 - val_loss: 1800.1271\n",
            "Epoch 240/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.6689 - val_loss: 1804.5609\n",
            "Epoch 241/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 12.5158 - val_loss: 1808.5842\n",
            "Epoch 242/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.3716 - val_loss: 1812.2769\n",
            "Epoch 243/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.2225 - val_loss: 1816.4554\n",
            "Epoch 244/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.0835 - val_loss: 1820.5529\n",
            "Epoch 245/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.9297 - val_loss: 1823.7704\n",
            "Epoch 246/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.7758 - val_loss: 1826.8669\n",
            "Epoch 247/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.6366 - val_loss: 1828.9382\n",
            "Epoch 248/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.4952 - val_loss: 1831.5356\n",
            "Epoch 249/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.3630 - val_loss: 1834.5137\n",
            "Epoch 250/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.2209 - val_loss: 1836.6973\n",
            "Epoch 251/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.0873 - val_loss: 1838.9620\n",
            "Epoch 252/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.9479 - val_loss: 1840.8634\n",
            "Epoch 253/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.8185 - val_loss: 1842.1848\n",
            "Epoch 254/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.6907 - val_loss: 1843.7633\n",
            "Epoch 255/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.5658 - val_loss: 1845.4056\n",
            "Epoch 256/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.4350 - val_loss: 1846.2839\n",
            "Epoch 257/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.3128 - val_loss: 1845.5546\n",
            "Epoch 258/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.1833 - val_loss: 1846.1680\n",
            "Epoch 259/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.0633 - val_loss: 1846.3113\n",
            "Epoch 260/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.9371 - val_loss: 1846.0225\n",
            "Epoch 261/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.8250 - val_loss: 1845.2402\n",
            "Epoch 262/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.6998 - val_loss: 1844.3158\n",
            "Epoch 263/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.5855 - val_loss: 1843.0409\n",
            "Epoch 264/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.4726 - val_loss: 1841.6445\n",
            "Epoch 265/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.3574 - val_loss: 1839.3405\n",
            "Epoch 266/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.2438 - val_loss: 1837.8971\n",
            "Epoch 267/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.1288 - val_loss: 1834.7452\n",
            "Epoch 268/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.0246 - val_loss: 1832.3002\n",
            "Epoch 269/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 8.9148 - val_loss: 1829.3822\n",
            "Epoch 270/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.8088 - val_loss: 1825.4808\n",
            "Epoch 271/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.6976 - val_loss: 1821.5391\n",
            "Epoch 272/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.5952 - val_loss: 1817.6709\n",
            "Epoch 273/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.4862 - val_loss: 1813.7913\n",
            "Epoch 274/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.3906 - val_loss: 1808.5101\n",
            "Epoch 275/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.2839 - val_loss: 1803.6989\n",
            "Epoch 276/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.1786 - val_loss: 1797.7791\n",
            "Epoch 277/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.0769 - val_loss: 1792.2173\n",
            "Epoch 278/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.9808 - val_loss: 1785.7725\n",
            "Epoch 279/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.8836 - val_loss: 1779.5416\n",
            "Epoch 280/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.7845 - val_loss: 1772.4606\n",
            "Epoch 281/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.6854 - val_loss: 1765.2760\n",
            "Epoch 282/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.5899 - val_loss: 1757.9623\n",
            "Epoch 283/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.4970 - val_loss: 1750.3749\n",
            "Epoch 284/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.4070 - val_loss: 1742.5809\n",
            "Epoch 285/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.3051 - val_loss: 1734.1348\n",
            "Epoch 286/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.2206 - val_loss: 1725.6613\n",
            "Epoch 287/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.1266 - val_loss: 1716.8717\n",
            "Epoch 288/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.0368 - val_loss: 1707.7596\n",
            "Epoch 289/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.9503 - val_loss: 1698.5573\n",
            "Epoch 290/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.8538 - val_loss: 1689.1157\n",
            "Epoch 291/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.7673 - val_loss: 1679.1357\n",
            "Epoch 292/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.6791 - val_loss: 1668.9076\n",
            "Epoch 293/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.5909 - val_loss: 1658.7269\n",
            "Epoch 294/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.5106 - val_loss: 1647.9312\n",
            "Epoch 295/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.4173 - val_loss: 1637.2635\n",
            "Epoch 296/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.3366 - val_loss: 1626.2218\n",
            "Epoch 297/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.2509 - val_loss: 1615.2499\n",
            "Epoch 298/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.1715 - val_loss: 1603.7424\n",
            "Epoch 299/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.0841 - val_loss: 1592.0750\n",
            "Epoch 300/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.0112 - val_loss: 1580.9767\n",
            "Epoch 301/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.9222 - val_loss: 1568.7057\n",
            "Epoch 302/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.8420 - val_loss: 1556.9047\n",
            "Epoch 303/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.7639 - val_loss: 1543.9268\n",
            "Epoch 304/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.6857 - val_loss: 1531.7096\n",
            "Epoch 305/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.6039 - val_loss: 1519.5935\n",
            "Epoch 306/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.5276 - val_loss: 1506.4503\n",
            "Epoch 307/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.4507 - val_loss: 1494.2811\n",
            "Epoch 308/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.3731 - val_loss: 1480.6761\n",
            "Epoch 309/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.2997 - val_loss: 1467.9264\n",
            "Epoch 310/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.2215 - val_loss: 1454.5154\n",
            "Epoch 311/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.1456 - val_loss: 1441.3442\n",
            "Epoch 312/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.0755 - val_loss: 1428.2965\n",
            "Epoch 313/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.9983 - val_loss: 1414.0807\n",
            "Epoch 314/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.9274 - val_loss: 1401.0226\n",
            "Epoch 315/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.8590 - val_loss: 1387.0514\n",
            "Epoch 316/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.7856 - val_loss: 1373.2725\n",
            "Epoch 317/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.7146 - val_loss: 1359.7012\n",
            "Epoch 318/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.6440 - val_loss: 1346.0320\n",
            "Epoch 319/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.5806 - val_loss: 1332.2167\n",
            "Epoch 320/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.5071 - val_loss: 1318.7064\n",
            "Epoch 321/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.4365 - val_loss: 1304.7642\n",
            "Epoch 322/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.3659 - val_loss: 1290.4807\n",
            "Epoch 323/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.3021 - val_loss: 1276.5134\n",
            "Epoch 324/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.2377 - val_loss: 1262.5938\n",
            "Epoch 325/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.1698 - val_loss: 1248.3691\n",
            "Epoch 326/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.1027 - val_loss: 1234.4585\n",
            "Epoch 327/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.0432 - val_loss: 1220.5225\n",
            "Epoch 328/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.9798 - val_loss: 1206.1268\n",
            "Epoch 329/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.9156 - val_loss: 1192.0359\n",
            "Epoch 330/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.8536 - val_loss: 1177.8582\n",
            "Epoch 331/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.7923 - val_loss: 1164.2690\n",
            "Epoch 332/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.7308 - val_loss: 1150.0864\n",
            "Epoch 333/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.6692 - val_loss: 1136.3723\n",
            "Epoch 334/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.6099 - val_loss: 1122.4933\n",
            "Epoch 335/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.5479 - val_loss: 1108.6458\n",
            "Epoch 336/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.4890 - val_loss: 1094.4657\n",
            "Epoch 337/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.4310 - val_loss: 1080.8335\n",
            "Epoch 338/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.3743 - val_loss: 1067.0383\n",
            "Epoch 339/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.3198 - val_loss: 1053.2474\n",
            "Epoch 340/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.2632 - val_loss: 1039.7194\n",
            "Epoch 341/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.2066 - val_loss: 1026.0632\n",
            "Epoch 342/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.1514 - val_loss: 1012.3239\n",
            "Epoch 343/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.1000 - val_loss: 999.1647\n",
            "Epoch 344/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.0448 - val_loss: 984.7725\n",
            "Epoch 345/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.9883 - val_loss: 971.3199\n",
            "Epoch 346/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.9345 - val_loss: 957.7491\n",
            "Epoch 347/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.8844 - val_loss: 944.6213\n",
            "Epoch 348/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.8327 - val_loss: 931.4850\n",
            "Epoch 349/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.7813 - val_loss: 918.2570\n",
            "Epoch 350/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.7299 - val_loss: 905.1246\n",
            "Epoch 351/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.6821 - val_loss: 891.8693\n",
            "Epoch 352/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.6331 - val_loss: 879.3646\n",
            "Epoch 353/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5876 - val_loss: 866.2457\n",
            "Epoch 354/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5392 - val_loss: 853.4647\n",
            "Epoch 355/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.4895 - val_loss: 840.8099\n",
            "Epoch 356/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.4434 - val_loss: 828.0967\n",
            "Epoch 357/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.3979 - val_loss: 815.6575\n",
            "Epoch 358/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.3531 - val_loss: 802.6373\n",
            "Epoch 359/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.3051 - val_loss: 790.5636\n",
            "Epoch 360/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.2635 - val_loss: 778.5208\n",
            "Epoch 361/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.2191 - val_loss: 766.2382\n",
            "Epoch 362/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 2.1766 - val_loss: 754.3631\n",
            "Epoch 363/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1340 - val_loss: 742.4800\n",
            "Epoch 364/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0919 - val_loss: 730.6633\n",
            "Epoch 365/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0509 - val_loss: 719.0568\n",
            "Epoch 366/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0096 - val_loss: 707.5395\n",
            "Epoch 367/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9696 - val_loss: 696.0409\n",
            "Epoch 368/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9300 - val_loss: 684.4302\n",
            "Epoch 369/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8926 - val_loss: 673.2682\n",
            "Epoch 370/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8528 - val_loss: 661.9013\n",
            "Epoch 371/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8152 - val_loss: 650.8429\n",
            "Epoch 372/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7779 - val_loss: 639.9631\n",
            "Epoch 373/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7412 - val_loss: 628.9249\n",
            "Epoch 374/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7070 - val_loss: 618.2849\n",
            "Epoch 375/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6691 - val_loss: 607.5009\n",
            "Epoch 376/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6356 - val_loss: 596.8285\n",
            "Epoch 377/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6009 - val_loss: 586.5054\n",
            "Epoch 378/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5665 - val_loss: 576.0573\n",
            "Epoch 379/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5330 - val_loss: 565.8104\n",
            "Epoch 380/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5001 - val_loss: 555.7657\n",
            "Epoch 381/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.4678 - val_loss: 545.7458\n",
            "Epoch 382/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.4343 - val_loss: 535.9328\n",
            "Epoch 383/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.4041 - val_loss: 526.0377\n",
            "Epoch 384/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.3742 - val_loss: 516.2005\n",
            "Epoch 385/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.3424 - val_loss: 506.4863\n",
            "Epoch 386/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.3133 - val_loss: 497.2434\n",
            "Epoch 387/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2844 - val_loss: 487.9053\n",
            "Epoch 388/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2552 - val_loss: 478.7556\n",
            "Epoch 389/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2283 - val_loss: 469.4646\n",
            "Epoch 390/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.1995 - val_loss: 460.5331\n",
            "Epoch 391/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.1713 - val_loss: 451.3397\n",
            "Epoch 392/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.1445 - val_loss: 442.7437\n",
            "Epoch 393/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.1176 - val_loss: 433.8347\n",
            "Epoch 394/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.0929 - val_loss: 425.3415\n",
            "Epoch 395/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.0667 - val_loss: 416.9361\n",
            "Epoch 396/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.0423 - val_loss: 408.4401\n",
            "Epoch 397/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.0170 - val_loss: 400.3289\n",
            "Epoch 398/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9928 - val_loss: 392.0838\n",
            "Epoch 399/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9695 - val_loss: 383.9164\n",
            "Epoch 400/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9467 - val_loss: 376.1518\n",
            "Epoch 401/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9236 - val_loss: 368.0417\n",
            "Epoch 402/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9011 - val_loss: 360.2408\n",
            "Epoch 403/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8797 - val_loss: 352.7191\n",
            "Epoch 404/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8581 - val_loss: 345.1613\n",
            "Epoch 405/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8373 - val_loss: 337.7373\n",
            "Epoch 406/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8168 - val_loss: 330.4716\n",
            "Epoch 407/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7960 - val_loss: 323.1825\n",
            "Epoch 408/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7762 - val_loss: 316.1994\n",
            "Epoch 409/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7559 - val_loss: 309.2650\n",
            "Epoch 410/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7375 - val_loss: 302.3896\n",
            "Epoch 411/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7190 - val_loss: 295.7498\n",
            "Epoch 412/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7018 - val_loss: 289.0027\n",
            "Epoch 413/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6836 - val_loss: 282.3650\n",
            "Epoch 414/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6662 - val_loss: 276.0255\n",
            "Epoch 415/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6476 - val_loss: 269.5826\n",
            "Epoch 416/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6312 - val_loss: 263.4134\n",
            "Epoch 417/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6149 - val_loss: 257.2120\n",
            "Epoch 418/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5988 - val_loss: 251.1453\n",
            "Epoch 419/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5833 - val_loss: 245.1430\n",
            "Epoch 420/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5675 - val_loss: 239.3022\n",
            "Epoch 421/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5530 - val_loss: 233.5435\n",
            "Epoch 422/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5381 - val_loss: 227.8792\n",
            "Epoch 423/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5239 - val_loss: 222.2097\n",
            "Epoch 424/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5097 - val_loss: 216.7616\n",
            "Epoch 425/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4954 - val_loss: 211.3799\n",
            "Epoch 426/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4817 - val_loss: 206.2108\n",
            "Epoch 427/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4688 - val_loss: 201.0652\n",
            "Epoch 428/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4560 - val_loss: 195.8802\n",
            "Epoch 429/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4432 - val_loss: 190.8967\n",
            "Epoch 430/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4313 - val_loss: 185.9284\n",
            "Epoch 431/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4191 - val_loss: 181.1295\n",
            "Epoch 432/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4070 - val_loss: 176.4017\n",
            "Epoch 433/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3960 - val_loss: 171.7346\n",
            "Epoch 434/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3850 - val_loss: 167.2139\n",
            "Epoch 435/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3742 - val_loss: 162.7473\n",
            "Epoch 436/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3634 - val_loss: 158.3781\n",
            "Epoch 437/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3529 - val_loss: 154.1183\n",
            "Epoch 438/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3424 - val_loss: 149.8698\n",
            "Epoch 439/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3327 - val_loss: 145.8119\n",
            "Epoch 440/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3231 - val_loss: 141.7669\n",
            "Epoch 441/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3140 - val_loss: 137.8431\n",
            "Epoch 442/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3048 - val_loss: 133.9664\n",
            "Epoch 443/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2962 - val_loss: 130.1750\n",
            "Epoch 444/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2869 - val_loss: 126.4954\n",
            "Epoch 445/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2784 - val_loss: 122.8590\n",
            "Epoch 446/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2702 - val_loss: 119.3463\n",
            "Epoch 447/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2622 - val_loss: 115.9063\n",
            "Epoch 448/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2539 - val_loss: 112.4004\n",
            "Epoch 449/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2467 - val_loss: 109.0721\n",
            "Epoch 450/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2392 - val_loss: 105.8672\n",
            "Epoch 451/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2318 - val_loss: 102.7044\n",
            "Epoch 452/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2248 - val_loss: 99.6000\n",
            "Epoch 453/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2179 - val_loss: 96.5782\n",
            "Epoch 454/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 93.6425\n",
            "Epoch 455/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2051 - val_loss: 90.7014\n",
            "Epoch 456/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1986 - val_loss: 87.9396\n",
            "Epoch 457/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1925 - val_loss: 85.1866\n",
            "Epoch 458/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1867 - val_loss: 82.5761\n",
            "Epoch 459/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1804 - val_loss: 79.9315\n",
            "Epoch 460/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 77.3655\n",
            "Epoch 461/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1695 - val_loss: 74.9941\n",
            "Epoch 462/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 72.5588\n",
            "Epoch 463/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1588 - val_loss: 70.2146\n",
            "Epoch 464/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1538 - val_loss: 67.9080\n",
            "Epoch 465/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1491 - val_loss: 65.6924\n",
            "Epoch 466/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1443 - val_loss: 63.5597\n",
            "Epoch 467/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1398 - val_loss: 61.4160\n",
            "Epoch 468/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1352 - val_loss: 59.3384\n",
            "Epoch 469/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1312 - val_loss: 57.3790\n",
            "Epoch 470/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 55.4319\n",
            "Epoch 471/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1226 - val_loss: 53.5478\n",
            "Epoch 472/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1190 - val_loss: 51.7074\n",
            "Epoch 473/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 49.9074\n",
            "Epoch 474/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1113 - val_loss: 48.2301\n",
            "Epoch 475/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1080 - val_loss: 46.5462\n",
            "Epoch 476/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 44.9184\n",
            "Epoch 477/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 43.3304\n",
            "Epoch 478/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 41.8005\n",
            "Epoch 479/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 40.2814\n",
            "Epoch 480/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 38.8913\n",
            "Epoch 481/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0888 - val_loss: 37.4978\n",
            "Epoch 482/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 36.1517\n",
            "Epoch 483/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 34.8226\n",
            "Epoch 484/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 33.5325\n",
            "Epoch 485/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 32.2934\n",
            "Epoch 486/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0757 - val_loss: 31.1050\n",
            "Epoch 487/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 29.9719\n",
            "Epoch 488/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0710 - val_loss: 28.8067\n",
            "Epoch 489/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 27.7617\n",
            "Epoch 490/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 26.7146\n",
            "Epoch 491/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0642 - val_loss: 25.7042\n",
            "Epoch 492/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 24.6892\n",
            "Epoch 493/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 23.7957\n",
            "Epoch 494/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 22.8439\n",
            "Epoch 495/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 21.9908\n",
            "Epoch 496/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 21.1160\n",
            "Epoch 497/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 20.2905\n",
            "Epoch 498/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 19.4789\n",
            "Epoch 499/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 18.7237\n",
            "Epoch 500/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 17.9671\n",
            "Epoch 501/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 17.2535\n",
            "Epoch 502/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 16.5647\n",
            "Epoch 503/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 15.9211\n",
            "Epoch 504/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 15.2252\n",
            "Epoch 505/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 14.6066\n",
            "Epoch 506/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 14.0159\n",
            "Epoch 507/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 13.4439\n",
            "Epoch 508/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 12.8808\n",
            "Epoch 509/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 12.3397\n",
            "Epoch 510/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 11.8579\n",
            "Epoch 511/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 11.3331\n",
            "Epoch 512/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 10.8208\n",
            "Epoch 513/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 10.3721\n",
            "Epoch 514/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 9.9193\n",
            "Epoch 515/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 9.5224\n",
            "Epoch 516/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 9.0766\n",
            "Epoch 517/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 8.6938\n",
            "Epoch 518/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 8.3064\n",
            "Epoch 519/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 7.9308\n",
            "Epoch 520/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 7.5596\n",
            "Epoch 521/800\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 7.2515\n",
            "Epoch 522/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 6.8837\n",
            "Epoch 523/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 6.5880\n",
            "Epoch 524/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 6.2629\n",
            "Epoch 525/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 5.9740\n",
            "Epoch 526/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 5.7144\n",
            "Epoch 527/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 5.4214\n",
            "Epoch 528/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 5.1772\n",
            "Epoch 529/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 4.9155\n",
            "Epoch 530/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 4.6685\n",
            "Epoch 531/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 4.4588\n",
            "Epoch 532/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 4.2297\n",
            "Epoch 533/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 4.0311\n",
            "Epoch 534/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 3.8171\n",
            "Epoch 535/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 3.6198\n",
            "Epoch 536/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0202 - val_loss: 3.4313\n",
            "Epoch 537/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 3.2693\n",
            "Epoch 538/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 3.0858\n",
            "Epoch 539/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 2.9066\n",
            "Epoch 540/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 2.7736\n",
            "Epoch 541/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0177 - val_loss: 2.6238\n",
            "Epoch 542/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 2.4820\n",
            "Epoch 543/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 2.3317\n",
            "Epoch 544/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 2.2142\n",
            "Epoch 545/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 2.0855\n",
            "Epoch 546/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0169 - val_loss: 1.9675\n",
            "Epoch 547/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0164 - val_loss: 1.8604\n",
            "Epoch 548/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 1.7395\n",
            "Epoch 549/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0166 - val_loss: 1.6496\n",
            "Epoch 550/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0156 - val_loss: 1.5330\n",
            "Epoch 551/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0155 - val_loss: 1.4420\n",
            "Epoch 552/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0157 - val_loss: 1.3607\n",
            "Epoch 553/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 1.2972\n",
            "Epoch 554/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 1.1987\n",
            "Epoch 555/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0143 - val_loss: 1.1250\n",
            "Epoch 556/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 1.0357\n",
            "Epoch 557/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 1.0025\n",
            "Epoch 558/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.9161\n",
            "Epoch 559/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.8579\n",
            "Epoch 560/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.8048\n",
            "Epoch 561/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.7525\n",
            "Epoch 562/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.6976\n",
            "Epoch 563/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.6497\n",
            "Epoch 564/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.6028\n",
            "Epoch 565/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0138 - val_loss: 0.5639\n",
            "Epoch 566/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0138 - val_loss: 0.5198\n",
            "Epoch 567/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0144 - val_loss: 0.4891\n",
            "Epoch 568/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.4504\n",
            "Epoch 569/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.4254\n",
            "Epoch 570/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.3872\n",
            "Epoch 571/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0137 - val_loss: 0.3605\n",
            "Epoch 572/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0140 - val_loss: 0.3310\n",
            "Epoch 573/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.3116\n",
            "Epoch 574/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.2903\n",
            "Epoch 575/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.2535\n",
            "Epoch 576/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.2493\n",
            "Epoch 577/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.2179\n",
            "Epoch 578/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.2164\n",
            "Epoch 579/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.1869\n",
            "Epoch 580/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.1876\n",
            "Epoch 581/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.1665\n",
            "Epoch 582/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.1580\n",
            "Epoch 583/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.1425\n",
            "Epoch 584/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.1367\n",
            "Epoch 585/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.1260\n",
            "Epoch 586/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.1216\n",
            "Epoch 587/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.1065\n",
            "Epoch 588/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.1017\n",
            "Epoch 589/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0931\n",
            "Epoch 590/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0846\n",
            "Epoch 591/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0830\n",
            "Epoch 592/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0750\n",
            "Epoch 593/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0682\n",
            "Epoch 594/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0678\n",
            "Epoch 595/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.0605\n",
            "Epoch 596/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0589\n",
            "Epoch 597/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0560\n",
            "Epoch 598/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0540\n",
            "Epoch 599/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0482\n",
            "Epoch 600/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.0517\n",
            "Epoch 601/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0138 - val_loss: 0.0442\n",
            "Epoch 602/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0411\n",
            "Epoch 603/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0390\n",
            "Epoch 604/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0384\n",
            "Epoch 605/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0379\n",
            "Epoch 606/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0355\n",
            "Epoch 607/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0345\n",
            "Epoch 608/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0326\n",
            "Epoch 609/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0313\n",
            "Epoch 610/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0299\n",
            "Epoch 611/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0311\n",
            "Epoch 612/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0275\n",
            "Epoch 613/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0291\n",
            "Epoch 614/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0272\n",
            "Epoch 615/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0288\n",
            "Epoch 616/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0266\n",
            "Epoch 617/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0254\n",
            "Epoch 618/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0264\n",
            "Epoch 619/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0249\n",
            "Epoch 620/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0240\n",
            "Epoch 621/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0252\n",
            "Epoch 622/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0248\n",
            "Epoch 623/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0244\n",
            "Epoch 624/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0140 - val_loss: 0.0252\n",
            "Epoch 625/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.0239\n",
            "Epoch 626/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0231\n",
            "Epoch 627/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0223\n",
            "Epoch 628/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0235\n",
            "Epoch 629/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0218\n",
            "Epoch 630/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0230\n",
            "Epoch 631/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0215\n",
            "Epoch 632/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0216\n",
            "Epoch 633/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0221\n",
            "Epoch 634/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0217\n",
            "Epoch 635/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0230\n",
            "Epoch 636/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0214\n",
            "Epoch 637/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0225\n",
            "Epoch 638/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0224\n",
            "Epoch 639/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0220\n",
            "Epoch 640/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0239\n",
            "Epoch 641/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0220\n",
            "Epoch 642/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0221\n",
            "Epoch 643/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.0217\n",
            "Epoch 644/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0206\n",
            "Epoch 645/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0215\n",
            "Epoch 646/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0226\n",
            "Epoch 647/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0203\n",
            "Epoch 648/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0219\n",
            "Epoch 649/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0214\n",
            "Epoch 650/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0217\n",
            "Epoch 651/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0205\n",
            "Epoch 652/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0214\n",
            "Epoch 653/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0236\n",
            "Epoch 654/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0208\n",
            "Epoch 655/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0219\n",
            "Epoch 656/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0221\n",
            "Epoch 657/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0218\n",
            "Epoch 658/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0215\n",
            "Epoch 659/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0222\n",
            "Epoch 660/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0219\n",
            "Epoch 661/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0207\n",
            "Epoch 662/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0237\n",
            "Epoch 663/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0213\n",
            "Epoch 664/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0231\n",
            "Epoch 665/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0199\n",
            "Epoch 666/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0124 - val_loss: 0.0218\n",
            "Epoch 667/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0219\n",
            "Epoch 668/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0221\n",
            "Epoch 669/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0246\n",
            "Epoch 670/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.0228\n",
            "Epoch 671/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0212\n",
            "Epoch 672/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0211\n",
            "Epoch 673/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0227\n",
            "Epoch 674/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0229\n",
            "Epoch 675/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0225\n",
            "Epoch 676/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0222\n",
            "Epoch 677/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0227\n",
            "Epoch 678/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0205\n",
            "Epoch 679/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0222\n",
            "Epoch 680/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0218\n",
            "Epoch 681/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0227\n",
            "Epoch 682/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0240\n",
            "Epoch 683/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0213\n",
            "Epoch 684/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0233\n",
            "Epoch 685/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0213\n",
            "Epoch 686/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0228\n",
            "Epoch 687/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0211\n",
            "Epoch 688/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0228\n",
            "Epoch 689/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0230\n",
            "Epoch 690/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.0228\n",
            "Epoch 691/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.0229\n",
            "Epoch 692/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0231\n",
            "Epoch 693/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0236\n",
            "Epoch 694/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0228\n",
            "Epoch 695/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.0231\n",
            "Epoch 696/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0226\n",
            "Epoch 697/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0236\n",
            "Epoch 698/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0217\n",
            "Epoch 699/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0219\n",
            "Epoch 700/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0210\n",
            "Epoch 701/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0237\n",
            "Epoch 702/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0217\n",
            "Epoch 703/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0255\n",
            "Epoch 704/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0227\n",
            "Epoch 705/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0234\n",
            "Epoch 706/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0250\n",
            "Epoch 707/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0233\n",
            "Epoch 708/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0230\n",
            "Epoch 709/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0239\n",
            "Epoch 710/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0230\n",
            "Epoch 711/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0237\n",
            "Epoch 712/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0232\n",
            "Epoch 713/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0228\n",
            "Epoch 714/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0237\n",
            "Epoch 715/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0220\n",
            "Epoch 716/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0212\n",
            "Epoch 717/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0239\n",
            "Epoch 718/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0224\n",
            "Epoch 719/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0268\n",
            "Epoch 720/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0211\n",
            "Epoch 721/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0239\n",
            "Epoch 722/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0206\n",
            "Epoch 723/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0217\n",
            "Epoch 724/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0248\n",
            "Epoch 725/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0231\n",
            "Epoch 726/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0269\n",
            "Epoch 727/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0252\n",
            "Epoch 728/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0264\n",
            "Epoch 729/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0235\n",
            "Epoch 730/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0226\n",
            "Epoch 731/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0228\n",
            "Epoch 732/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0242\n",
            "Epoch 733/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0221\n",
            "Epoch 734/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0234\n",
            "Epoch 735/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0235\n",
            "Epoch 736/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0232\n",
            "Epoch 737/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0234\n",
            "Epoch 738/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0234\n",
            "Epoch 739/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0242\n",
            "Epoch 740/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0242\n",
            "Epoch 741/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0232\n",
            "Epoch 742/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0235\n",
            "Epoch 743/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0236\n",
            "Epoch 744/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0240\n",
            "Epoch 745/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0239\n",
            "Epoch 746/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0239\n",
            "Epoch 747/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0251\n",
            "Epoch 748/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0244\n",
            "Epoch 749/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0248\n",
            "Epoch 750/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0244\n",
            "Epoch 751/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0230\n",
            "Epoch 752/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0243\n",
            "Epoch 753/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0215\n",
            "Epoch 754/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0252\n",
            "Epoch 755/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0230\n",
            "Epoch 756/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0244\n",
            "Epoch 757/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0240\n",
            "Epoch 758/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0255\n",
            "Epoch 759/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0220\n",
            "Epoch 760/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0270\n",
            "Epoch 761/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0237\n",
            "Epoch 762/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0238\n",
            "Epoch 763/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0232\n",
            "Epoch 764/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0234\n",
            "Epoch 765/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0248\n",
            "Epoch 766/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0224\n",
            "Epoch 767/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0257\n",
            "Epoch 768/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0231\n",
            "Epoch 769/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0237\n",
            "Epoch 770/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0250\n",
            "Epoch 771/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0244\n",
            "Epoch 772/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0235\n",
            "Epoch 773/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0279\n",
            "Epoch 774/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.0239\n",
            "Epoch 775/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0244\n",
            "Epoch 776/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0216\n",
            "Epoch 777/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0250\n",
            "Epoch 778/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0242\n",
            "Epoch 779/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0262\n",
            "Epoch 780/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.0240\n",
            "Epoch 781/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0254\n",
            "Epoch 782/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0210\n",
            "Epoch 783/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0242\n",
            "Epoch 784/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0240\n",
            "Epoch 785/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0276\n",
            "Epoch 786/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0242\n",
            "Epoch 787/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0246\n",
            "Epoch 788/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0242\n",
            "Epoch 789/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0286\n",
            "Epoch 790/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0252\n",
            "Epoch 791/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0227\n",
            "Epoch 792/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0231\n",
            "Epoch 793/800\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0248\n",
            "Epoch 794/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0248\n",
            "Epoch 795/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0132 - val_loss: 0.0237\n",
            "Epoch 796/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0236\n",
            "Epoch 797/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.0252\n",
            "Epoch 798/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0262\n",
            "Epoch 799/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.0250\n",
            "Epoch 800/800\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0256\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtdUbgUd4Ifw"
      },
      "source": [
        "And this is how you can predict an output for any number of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaO-wCLl3Wd_",
        "outputId": "d2fd7db0-0381-4472-e76a-98a4d81fd2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[149.87997]\n",
            " [139.02762]\n",
            " [145.91675]\n",
            " ...\n",
            " [146.40329]\n",
            " [132.8652 ]\n",
            " [150.22269]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VhmPDN4Sva"
      },
      "source": [
        "### 1.1.2.7 Visualize Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bDCQffM4Zh2"
      },
      "source": [
        "It's time to see how your model's progress during the training, If all is good, you will find the validation loss decreasing without neither overfitting nor underfitting.\n",
        "\n",
        "Note that the loss here is used to indicate overfitting or underfitting, but loss shouldn't be regarded as an evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "CEZ7SdwI2e_D",
        "outputId": "8c9a9b26-44e1-4ffc-dedb-fb4d09be11fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.figure()\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gni9AWDueGU"
      },
      "source": [
        "## 1.1.3 Conclusion:\n",
        "\n",
        "That's it! Congratulations on training a linear regression model. \n",
        "\n",
        "Make sure you finish the second part of the assignment and deliver all the requirements for the submission.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}